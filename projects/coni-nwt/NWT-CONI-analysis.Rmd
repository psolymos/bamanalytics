---
title: "NWT CONI results"
output: pdf_document
---

This doc describes methods to estimate density from recognizer based detections.

## Logic

#### I. Outcomes at the site level:

1a) a site is unsuitable: we observe 0
1b) a site is suitable: we can observe 0 or >0

2a) a site is unoccupied (0 home range overlaps with it): we observe a 0
2b) if the site is occupied (at least one home range overlaps with the location): we observe 0 or >0

Levels 1 and 2 is of interest, this describes the population and we have some ideas about the distribution of overlapping home ranges based on the 10% sample. Up to this point we are looking at a zero-inflated Poisson distribution where we have lambda (expected value of overlapping home ranges) and delta (proportion of suitable sites), and our population size estimate is delta*lambda. But there is a bit more complexity. Although we have an idea of lambda (10% sample), delta is confounded with other sources of 0s.

#### II. Outcomes at the visit level:

3a) CONI is not active: we observe a 0

3b) CONI is active: we observe a 0 or >0 --> we estimated this conditional probability from survical models as a function of dat/time (can use it as offset)

4a) a site is not used (occupied but nobody home): we observe 0

4b) a site is used (it is occupied, but individuals are in and out): we observe 0 or >0

(The 5th layer is detectability but we use our EDR estimate to standardize for effective area and ignore this component in the modeling otherwise.)

The only thing that bothers me is depending on the spacing of the sites and home range sizes, we are estimating the population size or the superpopulation size (with double counting the same individuals if sites are too close). Estimates of home range sizes might help thin the data to avoid double counting or at least to conduct a sensitivity analysis.


```{r mermaid}
library(DiagrammeR)
mermaid("
graph LR
A((Site is))-->B[Unsuitable: 0]
A-->C[Suitable]
C-->D[Unoccupied: 0]
C-->E[Occupied]
E-->F[Inactive: 0]
E-->G[Active]
G-->H[Unused: 0]
G-->I[Used]
I-->J[Undetected: 0]
I-->K[Detected: >0]

")
```


```{r preamble,include=FALSE}
library(mefa4)
library(lme4)
library(pbapply)

source("zi.fit.R")
source("mvocc.R")

load("~/GoogleWork/collaborations/coni-nwt/CONI-AB-NWT-datawithpredictors.RData")
dat$key <- gsub(" ", "+", paste0(dat$station, "+", dat$datetime))
x <- read.csv("~/GoogleWork/collaborations/coni-nwt/CONI-AB-NWT-withOffsets.csv")
all(dat$key == x$key)
dat <- data.frame(dat[,c("detection", "Offset", "station", "datetime", "nalc")], 
    x[,c("lat", "long", "A", "p", "q")])

## reclass NALC
levs <- c("1"="ConTmp",
    "2"="ConTai",
    "5"="Dec",
    "6"="Mix",
    "8"="Shr",
    "10"="Grs",
    "14"="Wet",
    "16"="Bar",
    "18"="Wat")
dat$alc <- dat$nalc
levels(dat$alc) <- levs[levels(dat$nalc)]

levs2 <- c("1"="ConTmp",
    "2"="ConTai",
    "5"="DM",
    "6"="DM",
    "8"="OP",
    "10"="OP",
    "14"="OP",
    "16"="OP",
    "18"="OP")
dat$lc <- dat$nalc
levels(dat$lc) <- levs2[levels(dat$lc)]

## study areas
dat$latc <- cut(dat$lat, c(0, 56, 60, 90))
dat$longc <- cut(dat$long, c(-180, -117, -114, 0))
dat$prov <- as.factor(ifelse(dat$longc == "(-114,0]", "AB", "NT"))
dat$treat <- factor("NAT", c("FR", "NAT"))
dat$treat[dat$prov == "NT" & dat$longc == "(-117,-114]"] <- "FR"
dat$treat[dat$prov == "AB" & dat$latc == "(56,60]"] <- "FR"
dat$sa <- as.factor(paste0(dat$prov, "-", dat$treat))
table(dat$sa)
plot(lat ~ long, dat, col=as.integer(dat$sa), pch=19)
```

## True (latent) occupancy and abundance

We use the 10% validated data from ECK.

Randomly selected 10% of the recordings scanned.
Filtered out recordings that had booms detected in them.
Reviewed all recognizer detections for those recordings.
Assigned individual ID to each boom detection in those recordings.
There's definitely more than one individual in some of these recordings.
Here's the distribution of the results:

- 1 CONI: 29 sites
- 2 CONI: 11 sites
- 3 CONI: 2 sites
- 4 CONI: 1 site

The one thing to keep in mind is that these are almost entirely
from the Alberta fire site. I suspect the densities will be lower at the
other province/treatment combos.

Fitting a conditional likelihood model to home range overlap data.
This follows Solymos et al. 2012 (Environmetrics). 

**Conditional maximum likelihood** --- Let $Y$ be a 
random variable, and $y$ are observations.
A zero inflated (ZI) distribution with 
non-zero inflated density function $f(y; \theta)$ can be written as
$P(Y=0) = \phi + (1-\phi) f(0; \theta)$; $P(Y=y) = (1-\phi) f(y; \theta)$; $y>0$,
where $\theta$ is a vector of model parameters, not including the ZI
parameter $\phi$ that is the probability of observing 0 as part of the
ZI process.

Now the probability mass function for the $>0$ counts can be written as
$P(Y=y \mid Y>0) = P(Y=y) / [1 - P(Y=0)]$.
The denominator can also be written as 
$1 - P(Y=0) = (1 - \phi) [1 - f(0; \theta)]$
Thus
$P(Y=y \mid Y>0) = \frac{(1-\phi) f(y; \theta)}{(1 - \phi) [1 - f(0; \theta)]} = \frac{f(y; \theta)}{1 - f(0; \theta)}$.

This conditional mass function can be used in estimating conditional maximum 
likelihood estimates of $\hat{\theta}^{(1)}$ based on the non-zero part of the data
($y^{(1)}$).

#### Poisson

$$ f(y; \theta) = f(y; \lambda) = e^{-\lambda} \frac{\lambda^{y}}{y!} $$
$$ P(Y=0) = \phi + (1 + \phi) e^{-\lambda} $$

#### Negative Binomial

$\gamma$ is Gamma variance in the Poisson-Gamma mixture parametrization
($A \sim Bernoulli(1 - \phi)$; $Y \sim Poisson(A u \lambda)$; 
$u \sim Gamma(mean=1, variance=\gamma)$):

$$ f(y; \theta) = f(y; \lambda,\gamma) = \frac{\Gamma (y + \gamma^{-1})}{\Gamma (\gamma^{-1})} \frac{(\gamma \lambda)^{y}}{(1+\gamma \lambda)^{(y + \gamma{-1})}} $$
$$ P(Y=0) = \phi + (1 + \phi) (1+\gamma \lambda)^{(\gamma{-1})} $$

This is all (and more) implemented in the `zi.fit` function.
We don't bother with offsets because this was based on looking
at multiple visits and counting total number of inds that used that site
over the entire time

```{r}
## number of inds at each site based on 10% of all sites by ECK
Y1 <- rep(1:4, c(29, 11, 2, 1)) # mean 1.418605
X1 <- data.matrix(rep(1, length(Y1)))
Z1 <- X1

cl0p <- zi.fit(Y1, X1, Z1, distr="pois", type="CL", hessian=TRUE)$CL
cl0nb <- zi.fit(Y1, X1, Z1, distr="negbin", type="CL", hessian=TRUE)$CL

## calculate AIC
logLik.default <- function(object, ...) 
    structure(object$loglik,
        df = length(object$coef),
        class = "logLik")
ic <- AIC(cl0p, cl0nb)
ic$BIC <- AIC(cl0p, cl0nb, k=log(length(Y1)))$AIC
ic$AICc <- ic$AIC + (2*ic$df^2+2*ic$df) / (length(Y1)-ic$df-1)
ic  # Poisson is better supported

## mean of the poisson (including 0 counts and offsets too)
(lambda <- exp(cl0p$coef))
## P(N=0) based on Poisson, which is the suitable but unoccupied probability
exp(-lambda)
## suitable and occupied
1-exp(-lambda)

cnt <- 0:(max(Y1)+1)
dp <- dpois(cnt, exp(cl0p$coef))
names(dp) <- cnt
names(dp)[length(dp)] <- paste0(cnt[length(dp)]-1, "+")
dp[length(dp)] <- 1-sum(dp[-length(dp)])

tab <- cbind(
    observed=c(NA, (1-sum(dp[-c(1, length(dp))]))*table(Y1)/length(Y1), NA),
    expected=dp)
rownames(tab) <- names(dp)
round(tab, 3)

tmp <- tab[-1,]
tmp[is.na(tmp)] <- 0
cs <- rbind("0"=c(0,0), apply(tmp, 2, cumsum))
cs[,1] <- cs[,1]/max(cs[,1])
cs[,2] <- cs[,2]/max(cs[,2])
plot(cs,type="l", col="grey", xlim=c(0,1), ylim=c(0,1))
text(cs, labels = rownames(tab))
abline(0,1,lty=2)

tmp <- barplot(tab[,1], ylim=c(0, 0.5))
lines(tmp, tab[,2], type="b")
```


Now we have individuals counted at each study areas:

```{r}
abu <- read.csv("~/GoogleWork/collaborations/coni-nwt/data/AbudanceEstimation.csv")
abu$sa <- as.factor(abu$sa)
table(abu$sa)

Y1 <- abu$abundance
X0 <- data.matrix(rep(1, length(Y1)))
Z0 <- X0
X1 <- model.matrix(~ sa, abu)

cl0p <- zi.fit(Y1, X0, Z0, distr="pois", type="CL", hessian=TRUE)$CL
cl1p <- zi.fit(Y1, X1, Z0, distr="pois", type="CL", hessian=TRUE)$CL
cl0nb <- zi.fit(Y1, X0, Z0, distr="negbin", type="CL", hessian=TRUE)$CL
cl1nb <- zi.fit(Y1, X1, Z0, distr="negbin", type="CL", hessian=TRUE)$CL

ic <- AIC(cl0p, cl1p, cl0nb, cl1nb)
ic$BIC <- AIC(cl0p, cl1p, cl0nb, cl1nb, k=log(length(Y1)))$AIC
ic$AICc <- ic$AIC + (2*ic$df^2+2*ic$df) / (length(Y1)-ic$df-1)
ic <- ic[order(ic$AICc),]
ic


## mean of the poisson (including 0 counts and offsets too)
Xp <- X1[!duplicated(abu$sa),]
rownames(Xp) <- abu$sa[!duplicated(abu$sa)]

round(lambda <- exp(drop(Xp %*% cl1p$coef)), 4)
## P(N=0) based on Poisson, which is the suitable but unoccupied probability
round(exp(-lambda), 4)
## suitable and occupied
round(1-exp(-lambda), 4)
```

```{r}
y <- matrix(NA, nlevels(dat$station), 30)
rownames(y) <- levels(dat$station)
p <- y
for (i in rownames(y)) {
    tmp <- dat[dat$station == i,]
    tmp <- tmp[order(as.character(tmp$datetime)),]
    y[i, seq_len(nrow(tmp))] <- tmp$detection
    p[i, seq_len(nrow(tmp))] <- tmp$p
}
dat2 <- nonDuplicated(dat, station, TRUE)
dat2 <- dat2[rownames(y),]
dat2$lc2 <- dat2$lc
levels(dat2$lc2) <- c("Con", "Con", "DMO", "DMO")
table(dat2$lc, dat2$lc2)

M <- nrow(y)
J <- ncol(y)
X <- matrix(1, M, 1)
Z <- matrix(1, M*J, 1)
X2 <- model.matrix(~lc2, dat2)
X4 <- model.matrix(~lc, dat2)
Z2 <- NULL
Z4 <- NULL
for (i in seq_len(ncol(y))) {
    Z2 <- rbind(Z2, X2)
    Z4 <- rbind(Z4, X4)
}
Aeff <- dat$A[1]

## lambda by study area
#lamvec <- lambda[1]
lamvec <- lambda[match(dat2$sa, names(lambda))]

# constant
#method <- "Nelder-Mead"
method <- "DE"
o00 <- mvocc(y, X, Z, p, lamvec, method=method)
o20 <- mvocc(y, X2, Z, p, lamvec, method=method)
o02 <- mvocc(y, X, Z2, p, lamvec, method=method)
o22 <- mvocc(y, X2, Z2, p, lamvec, method=method)
o40 <- mvocc(y, X4, Z, p, lamvec, method=method)
o04 <- mvocc(y, X, Z4, p, lamvec, method=method)
o44 <- mvocc(y, X4, Z4, p, lamvec, method=method)
o42 <- mvocc(y, X4, Z2, p, lamvec, method=method)
o24 <- mvocc(y, X2, Z4, p, lamvec, method=method)

aic <- AIC(o00,o20,o02,o22,o40,o04,o44,o42,o24)
aic$BIC <- BIC(o00,o20,o02,o22,o40,o04,o44,o42,o24)$BIC
aic$AICc <- aic$AIC + (2*aic$df^2+2*aic$df) / (nobs(o00)-aic$df-1)
aic$dAICc <- aic$AICc - min(aic$AICc)
aic <- aic[order(aic$dAICc),]
aic

summary(o22)
o22$coef
delta <- plogis(drop(X2 %*% o22$coef[1:2]))
q <- plogis(drop(X2 %*% o22$coef[3:4]))
summary(delta)
summary(q)

# cond probs
summary(delta) # suitable
delta[!duplicated(dat2$sa)]
summary(1-exp(-lambda)) # occupied
summary(p) # active
summary(q) # used

# pop size at the sites
s <- mefa4::sum_by(delta*lamvec, dat2$sa)
round(s[,"x"], 4)
# density
round(s[,"x"]/Aeff, 4)  # inds/ha

mean(delta*lamvec / Aeff) # inds/ha

V <- delta*lamvec / Aeff
c(All=round(mean(V), 4),
"AB-FR"=round(mean(V[dat2$sa=="AB-FR"]), 4),
"AB-NAT"=round(mean(V[dat2$sa=="AB-NAT"]), 4),
"NT-FR"=round(mean(V[dat2$sa=="NT-FR"]), 4),
"NT-NAT"=round(mean(V[dat2$sa=="NT-NAT"]), 4))


# how many ha to support 1 individual
1/(dhat * lambda / Aeff)

## --

plogis(o00$coef) # delta (prob of nonzero) and q (prob of use)
delta <- plogis(o00$coef[1])
q <- plogis(o00$coef[2])

# cond probs
delta # suitable
1-exp(-lambda) # occupied
mean(p, na.rm=TRUE) # active
q # used

# pop size at the sites
delta*lambda * nlevels(dat$station)
# density
delta*lambda / Aeff # inds/ha

lambda / Aeff # inds/ha

dhat <- structure(plogis(o42$coef[1] + c(0, o42$coef[2:4])), names=levels(dat2$lc))
puhat <- structure(plogis(o42$coef[5] + c(0, o42$coef[6])), names=levels(dat2$lc2))

mean(dhat[as.integer(dat2$lc)] * lambda)

# how many ha to support 1 individual
1/(dhat * lambda / Aeff)

## note: lambda represents AB fire site: how to generalize to other sites?
```


